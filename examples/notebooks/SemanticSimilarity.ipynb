{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c02b0509",
   "metadata": {},
   "source": [
    "# Semantic Similarity Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2eac35f8",
   "metadata": {},
   "source": [
    "## Setup imports and API keys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5edba05a",
   "metadata": {},
   "source": [
    "First, we'll need to set our API keys. If we are in DEBUG mode, we don't need to use real OpenAI or Hegel AI API keys, so for now we'll set them to empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DEBUG']=\"1\"\n",
    "os.environ['HEGELAI_API_KEY'] = \"\"\n",
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df3934db",
   "metadata": {},
   "source": [
    "Then we'll import the relevant `prompttools` modules to setup our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0841dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from prompttools.harness.prompt_template_harness import (\n",
    "    PromptTemplateExperimentationHarness,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b106a04",
   "metadata": {},
   "source": [
    "## Run experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e43545ff",
   "metadata": {},
   "source": [
    "Next, we create our test inputs. For this example, we'll use a prompt template, which uses [jinja](https://jinja.palletsprojects.com/en/3.1.x/) for templating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates = [\"Echo the following input: {{input}}\", \"Repeat the following input: {{input}}\"]\n",
    "user_inputs = [{\"input\": \"This is a test\"}, {\"input\": \"This is not a test\"}]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d152fdf",
   "metadata": {},
   "source": [
    "Now we can define an experimentation harness for our inputs and model. We could also pass model arguments if, for example, we wanted to change the model temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3086e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "harness = PromptTemplateExperimentationHarness(\"gpt-3.5-turbo\", prompt_templates, user_inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6f5463a",
   "metadata": {},
   "source": [
    "We can then run the experiment to get results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84304957",
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.prepare()\n",
    "harness.run()\n",
    "harness.visualize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b35caa9a",
   "metadata": {},
   "source": [
    "You can use the `pivot` keyword argument to view results by the template and inputs that created them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f1bde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "harness.visualize(pivot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "266c13eb",
   "metadata": {},
   "source": [
    "## Evaluate the model response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bebb8023",
   "metadata": {},
   "source": [
    "To evaluate the results, we'll define an eval function. Since we are prompting the model to echo our input, we can use semantic distance to check if the model's response is similar to the user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de3014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "\n",
    "def extract_responses(output) -> str:\n",
    "    return [choice[\"text\"] for choice in output[\"choices\"]]\n",
    "\n",
    "\n",
    "# Define an evaluation function that assigns scores to each inference\n",
    "def check_similarity(input_pair: Tuple[str,Dict[str,str]], results: Dict, metadata: Dict) -> float:\n",
    "    chroma_client = chromadb.Client()\n",
    "    collection = chroma_client.create_collection(name=\"test_collection\")\n",
    "    collection.add(\n",
    "        documents=[dict(input_pair[1])['input']],\n",
    "        ids=[\"id1\"]\n",
    "    )\n",
    "    query_results = collection.query(\n",
    "        query_texts=extract_responses(results),\n",
    "        n_results=1\n",
    "    )\n",
    "    chroma_client.delete_collection(\"test_collection\")\n",
    "    return min(query_results['distances'])[0]\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a277d94",
   "metadata": {},
   "source": [
    "Let's test our similarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f8701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_similarity((prompt_templates[0], user_inputs[0]), {\"choices\": [{\"text\": \"This is a test\"}, {\"text\": \"This is a text\"}]}, {})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "974d6065",
   "metadata": {},
   "source": [
    "Finally, we can evaluate and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.evaluate(\"did_echo\", check_similarity, use_input_pairs=True)\n",
    "harness.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6173e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
