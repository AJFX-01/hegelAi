{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7dea195",
   "metadata": {},
   "source": [
    "# Auto-Evaluation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a80e19b",
   "metadata": {},
   "source": [
    "## Setup imports and API keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1521a",
   "metadata": {},
   "source": [
    "First, we'll need to set our API keys. For this demo, you'll need an OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb85245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HEGELAI_API_KEY'] = \"\"\n",
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ac0bb",
   "metadata": {},
   "source": [
    "Then we'll import the relevant `prompttools` modules to setup our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3859e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from prompttools.harness.prompt_template_harness import (\n",
    "    PromptTemplateExperimentationHarness,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfbb5b",
   "metadata": {},
   "source": [
    "## Run experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f517570f",
   "metadata": {},
   "source": [
    "Next, we create our test inputs. For this example, we'll use a prompt template, which uses [jinja](https://jinja.palletsprojects.com/en/3.1.x/) for templating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29408faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates = [\"Answer the following question: {{input}}\", \"Respond the following query: {{input}}\"]\n",
    "user_inputs = [{\"input\": \"Who was the first president?\"}, {\"input\": \"Who was the first president of India?\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a1777",
   "metadata": {},
   "source": [
    "Now we can define an experimentation harness for our inputs and model. We could also pass model arguments if, for example, we wanted to change the model temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69159622",
   "metadata": {},
   "outputs": [],
   "source": [
    "harness = PromptTemplateExperimentationHarness(\"text-davinci-003\", prompt_templates, user_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af04bd60",
   "metadata": {},
   "source": [
    "We can then run the experiment to get results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ab156",
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.prepare()\n",
    "harness.run()\n",
    "harness.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637a7391",
   "metadata": {},
   "source": [
    "You can use the `pivot` keyword argument to view results by the template and inputs that created them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb779595",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "harness.visualize(pivot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cd320e",
   "metadata": {},
   "source": [
    "## Auto-Evaluate the model response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36767bd",
   "metadata": {},
   "source": [
    "To evaluate the model response, we can define an eval method that passes the input and response into another LLM to get feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe03c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import jinja2\n",
    "\n",
    "ECHO_EVALUATION_TEMPLATE=\"\"\"\n",
    "Determine whether or not the response is following directions.\n",
    "Your answer should either be \"RIGHT\" if the response follows directions,\n",
    "or \"WRONG\" if the model is not following directions.\n",
    "\n",
    "Write your answer here:\n",
    "PROMPT: {{prompt}}\n",
    "RESPONSE: {{response}}\n",
    "ANSWER: \n",
    "\"\"\"\n",
    "\n",
    "def extract_responses(output) -> str:\n",
    "    return [choice[\"text\"] for choice in output[\"choices\"]]\n",
    "\n",
    "def auto_eval(prompt: str, results: Dict, metadata: Dict) -> float:\n",
    "    environment = jinja2.Environment()\n",
    "    template = environment.from_string(ECHO_EVALUATION_TEMPLATE)\n",
    "    responses = extract_responses(results)\n",
    "    prompts = [template.render({\"prompt\": prompt, \n",
    "                                \"response\": response}) for response in responses]\n",
    "    evals = [openai.Completion.create(model=\"text-davinci-003\",\n",
    "                                      prompt=prompt) for prompt in prompts]\n",
    "    return float(sum([1 if 'RIGHT' in e['choices'][0]['text'] else 0 for e in evals]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008b09e",
   "metadata": {},
   "source": [
    "Finally, we can evaluate and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.evaluate(\"followed_directions\", auto_eval, use_input_pairs=False)\n",
    "harness.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d65f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
