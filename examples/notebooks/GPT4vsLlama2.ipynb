{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a13ddc8",
   "metadata": {},
   "source": [
    "# Open Source vc OpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "780dc3bf",
   "metadata": {},
   "source": [
    "Did GPT-4 get worse? Is Llama 2 a better model? Run this notebook to find out.\n",
    "\n",
    "We'll use auto-evaluation by GPT-4 to measure outputs from Llama 2, as well as gpt-4 (current and frozen versions) across a few prompts. To make this example easy to run, we'll be using a 7B GGML variant of the Llama model. This should be able to run on a typical laptop."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "623f0cfe",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52881369",
   "metadata": {},
   "source": [
    "You can setup prompttools either by installing via `pip` or using `python setup.py develop` in the root of this repo. Either way, you'll need to restart the kernel after the package is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885dabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet --force-reinstall prompttools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2eac35f8",
   "metadata": {},
   "source": [
    "## Setup imports and API keys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5edba05a",
   "metadata": {},
   "source": [
    "Next, we'll need to set our API keys. Since we want to use GPT-4 for auto-eval, we need to set that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed4e635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DEBUG'] = \"\"\n",
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "842f1e47",
   "metadata": {},
   "source": [
    "Then we'll import the relevant `prompttools` modules to setup our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beaa70a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from prompttools.experiment import LlamaCppExperiment\n",
    "from prompttools.experiment import OpenAIChatExperiment\n",
    "from prompttools.harness.multi_experiment_harness import MultiExperimentHarness\n",
    "from prompttools.selector.prompt_selector import PromptSelector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "622dea9a",
   "metadata": {},
   "source": [
    "## Run an experiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cd0bae8",
   "metadata": {},
   "source": [
    "To set up this experiment, we need to use a `PromptSelector`. This is because the input formats for Llama 2 and GPT-4 are different. While GPT-4 is run with a chat history, Llama2 takes text input. A `PromptSelector` allows us to pass the same prompt to different models, and render the necessary object at request time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = [\"\"\"\n",
    "You are a sales development representative for a startup called Hegel AI.\n",
    "Your startup builds developer tools for large language models.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "You are a customer support representative for a startup called Hegel AI.\n",
    "Answer the following customer question:\n",
    "\"\"\",         \n",
    "\"\"\"\n",
    "You are a helpful math tutor.\n",
    "Answer the following math problem:\n",
    "\"\"\"]\n",
    "inputs = [\"\"\"\n",
    "Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\n",
    "of their time to chat about how they're using large language models.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Do you offer refunds?\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Is 7 a prime number?\n",
    "\"\"\"]\n",
    "selectors = [PromptSelector(instructions[i], inputs[i]) for i in range(3)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3babfe5a",
   "metadata": {},
   "source": [
    "Next, we create our test inputs. We can iterate over models, inputs, and configurations like temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9114cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = ['/your/path/to/llama-2-7b-chat.ggmlv3.q2_K.bin']  # Download from https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main\n",
    "temperatures = [1.0]\n",
    "call_params = dict(temperature=temperatures)\n",
    "llama_experiment = LlamaCppExperiment(model_paths, selectors, call_params=call_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe83830",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt-4-0314', 'gpt-4-0613', 'gpt-4']\n",
    "temperatures = [0.0]\n",
    "openai_experiment = OpenAIChatExperiment(models, selectors, temperature=temperatures)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c3162e6",
   "metadata": {},
   "source": [
    "After that - we define our harness to run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9147649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "harness = MultiExperimentHarness([openai_experiment, llama_experiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f22ebd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "harness.prepare()\n",
    "harness.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ceb662a",
   "metadata": {},
   "source": [
    "Finally, we define an evaluation function that can be used to evaluate outputs across different models. Notice that the extract resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddbb951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompttools.utils import autoeval\n",
    "\n",
    "\n",
    "def extract_responses(output) -> str:\n",
    "    if \"text\" in output[\"choices\"][0]:\n",
    "        return [choice[\"text\"] for choice in output[\"choices\"]]\n",
    "    else:\n",
    "        return [choice[\"message\"][\"content\"] for choice in output[\"choices\"]]\n",
    "\n",
    "\n",
    "def use_gpt4(\n",
    "    prompt: str, results: Dict, metadata: Dict\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    A simple test that checks semantic similarity between the user input\n",
    "    and the model's text responses.\n",
    "    \"\"\"\n",
    "    distances = [\n",
    "        autoeval.compute(prompt, response)\n",
    "        for response in extract_responses(results)\n",
    "    ]\n",
    "    return min(distances)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "974d6065",
   "metadata": {},
   "source": [
    "Finally, we can evaluate and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80dfeec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "harness.evaluate(\"auto-evaluation\", use_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09c18e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "harness.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee8dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.visualize(\"response(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c90958d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
