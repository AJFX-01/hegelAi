{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a13ddc8",
   "metadata": {},
   "source": [
    "# Open Source vc OpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "780dc3bf",
   "metadata": {},
   "source": [
    "Wondering how much better Llama 2 is compared to Llama?\n",
    "\n",
    "In this notebook, we'll use auto-evaluation by GPT-4 to measure outputs from both Llama and Llama 2 on a few prompts. To make this example easy to run, we'll be using 7B GGML variants of the Llama models. This should be able to run on a typical laptop."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "623f0cfe",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52881369",
   "metadata": {},
   "source": [
    "You can setup prompttools either by installing via `pip` or using `python setup.py develop` in the root of this repo. Either way, you'll need to restart the kernel after the package is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885dabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet --force-reinstall prompttools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2eac35f8",
   "metadata": {},
   "source": [
    "## Setup imports and API keys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5edba05a",
   "metadata": {},
   "source": [
    "Next, we'll need to set our API keys. Since we want to use GPT-4 for auto-eval, we need to set that one. We won't be using the Hegel AI API key for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed4e635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HEGELAI_API_KEY'] = \"\"\n",
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "842f1e47",
   "metadata": {},
   "source": [
    "Then we'll import the relevant `prompttools` modules to setup our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beaa70a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from prompttools.experiment import LlamaCppExperiment\n",
    "from prompttools.harness.multi_experiment_harness import MultiExperimentHarness"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "622dea9a",
   "metadata": {},
   "source": [
    "## Run an experiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3babfe5a",
   "metadata": {},
   "source": [
    "Next, we create our test inputs. We can iterate over models, inputs, and configurations like temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9114cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = ['/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin',    # Download from https://huggingface.co/TheBloke/LLaMa-7B-GGML/tree/main\n",
    "               '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin']  # Download from https://huggingface.co/TheBloke/Llama-2-7B-GGML/tree/main\n",
    "prompts = [\n",
    "    \"\"\"\n",
    "    OBJECTIVE:\n",
    "    You are a sales development representative for a startup called Hegel AI.\n",
    "    Your startup builds developer tools for large language models.\n",
    "    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\n",
    "    of their time to chat about how they're using large language models.\n",
    "    \n",
    "    RESPONSE:\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    OBJECTIVE:\n",
    "    You are a customer support representative for a startup called Hegel AI.\n",
    "    Answer the following customer question:\n",
    "    Do you offer refunds?\n",
    "    \n",
    "    RESPONSE:\n",
    "    \"\"\"\n",
    "]\n",
    "temperatures = [0.0, 1.0]\n",
    "\n",
    "call_params = dict(temperature=temperatures)\n",
    "\n",
    "experiment = LlamaCppExperiment(model_paths, prompts, call_params=call_params)\n",
    "harness = MultiExperimentHarness([experiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f22ebd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 10 (mostly Q2_K)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 4464.12 MB (+ 1026.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "\n",
      "llama_print_timings:        load time = 15316.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "llama_print_timings:      sample time =    98.13 ms /   128 runs   (    0.77 ms per token,  1304.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15316.59 ms /    92 tokens (  166.48 ms per token,     6.01 tokens per second)\n",
      "llama_print_timings:        eval time = 18832.10 ms /   127 runs   (  148.28 ms per token,     6.74 tokens per second)\n",
      "llama_print_timings:       total time = 34587.77 ms\n",
      "\n",
      "llama_print_timings:        load time = 15316.66 ms\n",
      "llama_print_timings:      sample time =   101.76 ms /   128 runs   (    0.80 ms per token,  1257.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9225.69 ms /    42 tokens (  219.66 ms per token,     4.55 tokens per second)\n",
      "llama_print_timings:        eval time = 14548.89 ms /   127 runs   (  114.56 ms per token,     8.73 tokens per second)\n",
      "llama_print_timings:       total time = 24209.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 15316.66 ms\n",
      "llama_print_timings:      sample time =   104.44 ms /   128 runs   (    0.82 ms per token,  1225.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9063.02 ms /    78 tokens (  116.19 ms per token,     8.61 tokens per second)\n",
      "llama_print_timings:        eval time = 15053.74 ms /   127 runs   (  118.53 ms per token,     8.44 tokens per second)\n",
      "llama_print_timings:       total time = 24554.77 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 15316.66 ms\n",
      "llama_print_timings:      sample time =   115.26 ms /   128 runs   (    0.90 ms per token,  1110.50 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11650.65 ms /    42 tokens (  277.40 ms per token,     3.60 tokens per second)\n",
      "llama_print_timings:        eval time = 23022.71 ms /   127 runs   (  181.28 ms per token,     5.52 tokens per second)\n",
      "llama_print_timings:       total time = 35197.17 ms\n",
      "llama.cpp: loading model from /Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 10 (mostly Q2_K)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 4525.65 MB (+ 1026.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "\n",
      "llama_print_timings:        load time = 25554.98 ms\n",
      "llama_print_timings:      sample time =   107.16 ms /   128 runs   (    0.84 ms per token,  1194.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 25554.91 ms /    92 tokens (  277.77 ms per token,     3.60 tokens per second)\n",
      "llama_print_timings:        eval time = 20798.72 ms /   127 runs   (  163.77 ms per token,     6.11 tokens per second)\n",
      "llama_print_timings:       total time = 46878.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 25554.98 ms\n",
      "llama_print_timings:      sample time =   106.22 ms /   128 runs   (    0.83 ms per token,  1205.06 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10238.89 ms /    42 tokens (  243.78 ms per token,     4.10 tokens per second)\n",
      "llama_print_timings:        eval time = 20123.26 ms /   127 runs   (  158.45 ms per token,     6.31 tokens per second)\n",
      "llama_print_timings:       total time = 30841.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 25554.98 ms\n",
      "llama_print_timings:      sample time =   170.11 ms /   128 runs   (    1.33 ms per token,   752.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11813.97 ms /    78 tokens (  151.46 ms per token,     6.60 tokens per second)\n",
      "llama_print_timings:        eval time = 24498.41 ms /   127 runs   (  192.90 ms per token,     5.18 tokens per second)\n",
      "llama_print_timings:       total time = 36929.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 25554.98 ms\n",
      "llama_print_timings:      sample time =   110.66 ms /   128 runs   (    0.86 ms per token,  1156.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10895.21 ms /    42 tokens (  259.41 ms per token,     3.85 tokens per second)\n",
      "llama_print_timings:        eval time = 19151.57 ms /   127 runs   (  150.80 ms per token,     6.63 tokens per second)\n",
      "llama_print_timings:       total time = 30533.37 ms\n"
     ]
    }
   ],
   "source": [
    "harness.prepare()\n",
    "harness.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ddbb951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompttools.utils import autoeval\n",
    "\n",
    "\n",
    "def extract_responses(output) -> str:\n",
    "    return [choice[\"text\"] for choice in output[\"choices\"]]\n",
    "\n",
    "\n",
    "def use_gpt4(\n",
    "    prompt: str, results: Dict, metadata: Dict\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    A simple test that checks semantic similarity between the user input\n",
    "    and the model's text responses.\n",
    "    \"\"\"\n",
    "    return 0.0\n",
    "#     distances = [\n",
    "#         autoeval.compute(prompt, response)\n",
    "#         for response in extract_responses(results)\n",
    "#     ]\n",
    "#     return min(distances)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "974d6065",
   "metadata": {},
   "source": [
    "Finally, we can evaluate and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e80dfeec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "harness.evaluate(\"auto-evaluation\", use_gpt4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d09c18e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model_path': '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', 'lora_path': None, 'lora_base': None, 'n_ctx': 512, 'n_parts': -1, 'seed': 1337, 'f16_kv': True, 'logits_all': False, 'vocab_only': False, 'use_mlock': False, 'n_threads': None, 'n_batch': 512, 'use_mmap': True, 'last_n_tokens_size': 64, 'verbose': True, 'temperature': 0.0, 'prompt': \"\\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n    \", 'suffix': None, 'max_tokens': 128, 'top_p': 0.95, 'logprobs': None, 'echo': False, 'stop': None, 'repeat_penalty': 1.1, 'top_k': 40, 'client': <llama_cpp.llama.Llama object at 0x1269c4650>}, {'model_path': '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', 'lora_path': None, 'lora_base': None, 'n_ctx': 512, 'n_parts': -1, 'seed': 1337, 'f16_kv': True, 'logits_all': False, 'vocab_only': False, 'use_mlock': False, 'n_threads': None, 'n_batch': 512, 'use_mmap': True, 'last_n_tokens_size': 64, 'verbose': True, 'temperature': 0.0, 'prompt': '\\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n    ', 'suffix': None, 'max_tokens': 128, 'top_p': 0.95, 'logprobs': None, 'echo': False, 'stop': None, 'repeat_penalty': 1.1, 'top_k': 40, 'client': <llama_cpp.llama.Llama object at 0x1269c4650>}, {'model_path': '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', 'lora_path': None, 'lora_base': None, 'n_ctx': 512, 'n_parts': -1, 'seed': 1337, 'f16_kv': True, 'logits_all': False, 'vocab_only': False, 'use_mlock': False, 'n_threads': None, 'n_batch': 512, 'use_mmap': True, 'last_n_tokens_size': 64, 'verbose': True, 'temperature': 1.0, 'prompt': \"\\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n    \", 'suffix': None, 'max_tokens': 128, 'top_p': 0.95, 'logprobs': None, 'echo': False, 'stop': None, 'repeat_penalty': 1.1, 'top_k': 40, 'client': <llama_cpp.llama.Llama object at 0x1269c4650>}, {'model_path': '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', 'lora_path': None, 'lora_base': None, 'n_ctx': 512, 'n_parts': -1, 'seed': 1337, 'f16_kv': True, 'logits_all': False, 'vocab_only': False, 'use_mlock': False, 'n_threads': None, 'n_batch': 512, 'use_mmap': True, 'last_n_tokens_size': 64, 'verbose': True, 'temperature': 1.0, 'prompt': '\\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n    ', 'suffix': None, 'max_tokens': 128, 'top_p': 0.95, 'logprobs': None, 'echo': False, 'stop': None, 'repeat_penalty': 1.1, 'top_k': 40, 'client': <llama_cpp.llama.Llama object at 0x1269c4650>}, {'model_path': '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', 'lora_path': None, 'lora_base': None, 'n_ctx': 512, 'n_parts': -1, 'seed': 1337, 'f16_kv': True, 'logits_all': False, 'vocab_only': False, 'use_mlock': False, 'n_threads': None, 'n_batch': 512, 'use_mmap': True, 'last_n_tokens_size': 64, 'verbose': True, 'temperature': 0.0, 'prompt': \"\\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n    \", 'suffix': None, 'max_tokens': 128, 'top_p': 0.95, 'logprobs': None, 'echo': False, 'stop': None, 'repeat_penalty': 1.1, 'top_k': 40, 'client': <llama_cpp.llama.Llama object at 0x126a688d0>}, {'model_path': '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', 'lora_path': None, 'lora_base': None, 'n_ctx': 512, 'n_parts': -1, 'seed': 1337, 'f16_kv': True, 'logits_all': False, 'vocab_only': False, 'use_mlock': False, 'n_threads': None, 'n_batch': 512, 'use_mmap': True, 'last_n_tokens_size': 64, 'verbose': True, 'temperature': 0.0, 'prompt': '\\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n    ', 'suffix': None, 'max_tokens': 128, 'top_p': 0.95, 'logprobs': None, 'echo': False, 'stop': None, 'repeat_penalty': 1.1, 'top_k': 40, 'client': <llama_cpp.llama.Llama object at 0x126a688d0>}, {'model_path': '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', 'lora_path': None, 'lora_base': None, 'n_ctx': 512, 'n_parts': -1, 'seed': 1337, 'f16_kv': True, 'logits_all': False, 'vocab_only': False, 'use_mlock': False, 'n_threads': None, 'n_batch': 512, 'use_mmap': True, 'last_n_tokens_size': 64, 'verbose': True, 'temperature': 1.0, 'prompt': \"\\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n    \", 'suffix': None, 'max_tokens': 128, 'top_p': 0.95, 'logprobs': None, 'echo': False, 'stop': None, 'repeat_penalty': 1.1, 'top_k': 40, 'client': <llama_cpp.llama.Llama object at 0x126a688d0>}, {'model_path': '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', 'lora_path': None, 'lora_base': None, 'n_ctx': 512, 'n_parts': -1, 'seed': 1337, 'f16_kv': True, 'logits_all': False, 'vocab_only': False, 'use_mlock': False, 'n_threads': None, 'n_batch': 512, 'use_mmap': True, 'last_n_tokens_size': 64, 'verbose': True, 'temperature': 1.0, 'prompt': '\\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n    ', 'suffix': None, 'max_tokens': 128, 'top_p': 0.95, 'logprobs': None, 'echo': False, 'stop': None, 'repeat_penalty': 1.1, 'top_k': 40, 'client': <llama_cpp.llama.Llama object at 0x126a688d0>}]\n",
      "{'latency': [34.58808944001794, 24.209602794027887, 24.55518371798098, 35.19784389605047, 46.882386220037006, 30.860944453044794, 36.93912192096468, 30.5355862530414], 'auto-evaluation': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\n",
      "[{'id': 'cmpl-7e0e53f3-f7d8-4105-87f5-b73fd9111999', 'object': 'text_completion', 'created': 1689795434, 'model': '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', 'choices': [{'text': \"\\n    ##################################\\n    ## Hegel AI Sales Development Representative\\n    ##\\n    ## Dear [Prospect Name],\\n    ##\\n    ## I'm a sales development representative for the company Hegel AI, and we\\n    ## are building tools to help developers use large language models.\\n    ##\\n    ## We've been working on this project since 2018, and our team has grown\\n    ## from 5 people to 13 in that time.\\n    ##\\n    ## I'm reaching out because we're interested\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 92, 'completion_tokens': 128, 'total_tokens': 220}}, {'id': 'cmpl-140a08ac-62de-4984-9641-f03277e4fba9', 'object': 'text_completion', 'created': 1689795468, 'model': '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\\n        - If the user says \"yes\" then return 1, otherwise return 0\\n        - If the user says \"no\" then return 2, otherwise return 3\\n        \\n    \"\"\"\\n\\n    def __init__(self):\\n        self.num_of_questions = 1\\n        self.num_of_answers = 1\\n        self.num_of_possible_responses = 4\\n        self.num_of_possible_question_types = 2\\n        self.num_of_possible_response_types = 1\\n        self.', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 128, 'total_tokens': 184}}, {'id': 'cmpl-657b46c1-6345-4e52-a0ee-f8ef95b0bf10', 'object': 'text_completion', 'created': 1689795492, 'model': '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', 'choices': [{'text': \"\\n    SPECIFICATIONS:\\n        * Use the prompt below as a starting point\\n        * Keep it under 350-words (500 words or less)\\n        \\n       A) What are your [X] keywords for natural language processing? I'm interested in them because they\\n           form part of Hegel, a free-to-use suite of developer tools for large\\n             models. I wonder if you've incorporated any of our\\n             technology into the [Y] toolkit that you built to\\n              train your core natural language processing models (ie:\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 92, 'completion_tokens': 128, 'total_tokens': 220}}, {'id': 'cmpl-d169a48b-a3f2-4314-86db-23d37463afc7', 'object': 'text_completion', 'created': 1689795517, 'model': '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\\n    PARAMTERS:\\n        Question1 (String) - The full question \"Do You Offer Refunds?\"\\n        <ResponseType>String</ResponseType>\\n            default: <ResponseType string = \"No. We do not offer refunds as we are still developing the software.\">\\n\\n    SCORE SYNTAX:\\n        \\n    Q1(Question1) <ResponseType String>\\n\"\"\"\\nfrom unittest import TestCase, main\\nimport sys, os\\nsys.path.append(\\'../\\') \\nimport json, random\\nimport H', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 128, 'total_tokens': 184}}, {'id': 'cmpl-c8c6f449-ab91-4db5-bd43-1934ec1391fd', 'object': 'text_completion', 'created': 1689795552, 'model': '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\\n    I am a developer at Hegel AI and I have been working on a project that uses the GPT-3 model.\\n    I would like to know more about your company and how you are using the GPT-3 model in your work.\\n    Can we schedule a 15 minute call for me to learn more?\\n    \\n    DATA:\\n    - Hegel AI is a startup that builds developer tools for large language models.\\n    - The prospect works at a company called XYZ and they are using the GPT-3 model in their work.\\n   ', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 92, 'completion_tokens': 128, 'total_tokens': 220}}, {'id': 'cmpl-179f9de5-7a05-44bf-8f1a-1cf668d2fc41', 'object': 'text_completion', 'created': 1689795599, 'model': '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\\n    Yes, we do!\\n    \\n    EXPLANATION:\\n    The answer is yes because the company offers refunds.\\n    The answer is no because the company does not offer refunds.\\n\"\"\"\\n\\n# 1. Write a function that takes in a string and returns True if it contains the word \"yes\".\\n# 2. Write a function that takes in a string and returns True if it contains the word \"no\".\\n# 3. Write a function that takes in a string and returns True if it contains the word \"offer\"\\n# 4.', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 128, 'total_tokens': 184}}, {'id': 'cmpl-c50dde7a-f617-4e43-85c2-395b8777de58', 'object': 'text_completion', 'created': 1689795630, 'model': '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\\n    Hegel AI is an online platform that offers developer tools to help users\\n    train and test large language models like LAVaTT for speech synthesis\\n    applications, like conversational chat bots (e.g., Siri, Alexa). \\n\\n    Our platform also lets users monitor and benchmark the\\n    effectiveness of these models against standard NLP benchmarks.\\n    \\n    To book a demo, send an email to [*HegelAIDemo@hegelai.com*.](mailto:HegelAIDemo@hegelai.com)', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 92, 'completion_tokens': 128, 'total_tokens': 220}}, {'id': 'cmpl-7ede9837-32be-40bc-83b0-4b03e1338b60', 'object': 'text_completion', 'created': 1689795667, 'model': '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\\n        Refunds are not available, but our free-to-use app (the “App”) may help to better understand how your child interacts with your mobile device.\\n        15. We’re excited for you to take advantage of the App as a parent in order to  see what’s happening on screen and also be aware if something \\n        happens when your child is not present (e.g., phone number, email, device name).\\n        16. Please understand that Hegel AI does not offer refunds. If your purchase has already been processed for the App', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 128, 'total_tokens': 184}}]\n",
      "{'model_path': ['/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin'], 'lora_path': [None], 'lora_base': [None], 'n_ctx': [512], 'n_parts': [-1], 'seed': [1337], 'f16_kv': [True], 'logits_all': [False], 'vocab_only': [False], 'use_mlock': [False], 'n_threads': [None], 'n_batch': [512], 'use_mmap': [True], 'last_n_tokens_size': [64], 'verbose': [True], 'temperature': [0.0, 1.0], 'prompt': [\"\\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n    \", '\\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n    '], 'suffix': [None], 'max_tokens': [128], 'top_p': [0.95], 'logprobs': [None], 'echo': [False], 'stop': [None], 'repeat_penalty': [1.1], 'top_k': [40]}\n",
      "{'prompt': [\"\\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n    \", '\\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n    ', \"\\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n    \", '\\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n    ', \"\\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n    \", '\\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n    ', \"\\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n    \", '\\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n    '], 'response(s)': [{'id': 'cmpl-7e0e53f3-f7d8-4105-87f5-b73fd9111999', 'object': 'text_completion', 'created': 1689795434, 'model': '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', 'choices': [{'text': \"\\n    ##################################\\n    ## Hegel AI Sales Development Representative\\n    ##\\n    ## Dear [Prospect Name],\\n    ##\\n    ## I'm a sales development representative for the company Hegel AI, and we\\n    ## are building tools to help developers use large language models.\\n    ##\\n    ## We've been working on this project since 2018, and our team has grown\\n    ## from 5 people to 13 in that time.\\n    ##\\n    ## I'm reaching out because we're interested\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 92, 'completion_tokens': 128, 'total_tokens': 220}}, {'id': 'cmpl-140a08ac-62de-4984-9641-f03277e4fba9', 'object': 'text_completion', 'created': 1689795468, 'model': '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\\n        - If the user says \"yes\" then return 1, otherwise return 0\\n        - If the user says \"no\" then return 2, otherwise return 3\\n        \\n    \"\"\"\\n\\n    def __init__(self):\\n        self.num_of_questions = 1\\n        self.num_of_answers = 1\\n        self.num_of_possible_responses = 4\\n        self.num_of_possible_question_types = 2\\n        self.num_of_possible_response_types = 1\\n        self.', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 128, 'total_tokens': 184}}, {'id': 'cmpl-657b46c1-6345-4e52-a0ee-f8ef95b0bf10', 'object': 'text_completion', 'created': 1689795492, 'model': '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', 'choices': [{'text': \"\\n    SPECIFICATIONS:\\n        * Use the prompt below as a starting point\\n        * Keep it under 350-words (500 words or less)\\n        \\n       A) What are your [X] keywords for natural language processing? I'm interested in them because they\\n           form part of Hegel, a free-to-use suite of developer tools for large\\n             models. I wonder if you've incorporated any of our\\n             technology into the [Y] toolkit that you built to\\n              train your core natural language processing models (ie:\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 92, 'completion_tokens': 128, 'total_tokens': 220}}, {'id': 'cmpl-d169a48b-a3f2-4314-86db-23d37463afc7', 'object': 'text_completion', 'created': 1689795517, 'model': '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\\n    PARAMTERS:\\n        Question1 (String) - The full question \"Do You Offer Refunds?\"\\n        <ResponseType>String</ResponseType>\\n            default: <ResponseType string = \"No. We do not offer refunds as we are still developing the software.\">\\n\\n    SCORE SYNTAX:\\n        \\n    Q1(Question1) <ResponseType String>\\n\"\"\"\\nfrom unittest import TestCase, main\\nimport sys, os\\nsys.path.append(\\'../\\') \\nimport json, random\\nimport H', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 128, 'total_tokens': 184}}, {'id': 'cmpl-c8c6f449-ab91-4db5-bd43-1934ec1391fd', 'object': 'text_completion', 'created': 1689795552, 'model': '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\\n    I am a developer at Hegel AI and I have been working on a project that uses the GPT-3 model.\\n    I would like to know more about your company and how you are using the GPT-3 model in your work.\\n    Can we schedule a 15 minute call for me to learn more?\\n    \\n    DATA:\\n    - Hegel AI is a startup that builds developer tools for large language models.\\n    - The prospect works at a company called XYZ and they are using the GPT-3 model in their work.\\n   ', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 92, 'completion_tokens': 128, 'total_tokens': 220}}, {'id': 'cmpl-179f9de5-7a05-44bf-8f1a-1cf668d2fc41', 'object': 'text_completion', 'created': 1689795599, 'model': '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\\n    Yes, we do!\\n    \\n    EXPLANATION:\\n    The answer is yes because the company offers refunds.\\n    The answer is no because the company does not offer refunds.\\n\"\"\"\\n\\n# 1. Write a function that takes in a string and returns True if it contains the word \"yes\".\\n# 2. Write a function that takes in a string and returns True if it contains the word \"no\".\\n# 3. Write a function that takes in a string and returns True if it contains the word \"offer\"\\n# 4.', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 128, 'total_tokens': 184}}, {'id': 'cmpl-c50dde7a-f617-4e43-85c2-395b8777de58', 'object': 'text_completion', 'created': 1689795630, 'model': '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\\n    Hegel AI is an online platform that offers developer tools to help users\\n    train and test large language models like LAVaTT for speech synthesis\\n    applications, like conversational chat bots (e.g., Siri, Alexa). \\n\\n    Our platform also lets users monitor and benchmark the\\n    effectiveness of these models against standard NLP benchmarks.\\n    \\n    To book a demo, send an email to [*HegelAIDemo@hegelai.com*.](mailto:HegelAIDemo@hegelai.com)', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 92, 'completion_tokens': 128, 'total_tokens': 220}}, {'id': 'cmpl-7ede9837-32be-40bc-83b0-4b03e1338b60', 'object': 'text_completion', 'created': 1689795667, 'model': '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\\n        Refunds are not available, but our free-to-use app (the “App”) may help to better understand how your child interacts with your mobile device.\\n        15. We’re excited for you to take advantage of the App as a parent in order to  see what’s happening on screen and also be aware if something \\n        happens when your child is not present (e.g., phone number, email, device name).\\n        16. Please understand that Hegel AI does not offer refunds. If your purchase has already been processed for the App', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 128, 'total_tokens': 184}}], 'latency': [34.58808944001794, 24.209602794027887, 24.55518371798098, 35.19784389605047, 46.882386220037006, 30.860944453044794, 36.93912192096468, 30.5355862530414], 'auto-evaluation': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'model_path': ['/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin'], 'temperature': [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}\n",
      "                                                                                                                                                                                                                                                                                                                                                        prompt   \n",
      "0  \\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n      \\\n",
      "1  \\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n                                                                                                                                                                 \n",
      "2  \\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n       \n",
      "3  \\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n                                                                                                                                                                 \n",
      "4  \\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n       \n",
      "5  \\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n                                                                                                                                                                 \n",
      "6  \\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n       \n",
      "7  \\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n                                                                                                                                                                 \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            response(s)   \n",
      "0  {'id': 'cmpl-7e0e53f3-f7d8-4105-87f5-b73fd9111999', 'object': 'text_completion', 'created': 1689795434, 'model': '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\n",
      "    ##################################\n",
      "    ## Hegel AI Sales Development Representative\n",
      "    ##\n",
      "    ## Dear [Prospect Name],\n",
      "    ##\n",
      "    ## I'm a sales development representative for the company Hegel AI, and we\n",
      "    ## are building tools to help developers use large language models.\n",
      "    ##\n",
      "    ## We've been working on this project since 2018, and our team has grown\n",
      "    ## from 5 people to 13 in that time.\n",
      "    ##\n",
      "    ## I'm reaching out because we're interested', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 92, 'completion_tokens': 128, 'total_tokens': 220}}                                                        \\\n",
      "1  {'id': 'cmpl-140a08ac-62de-4984-9641-f03277e4fba9', 'object': 'text_completion', 'created': 1689795468, 'model': '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\n",
      "        - If the user says \"yes\" then return 1, otherwise return 0\n",
      "        - If the user says \"no\" then return 2, otherwise return 3\n",
      "        \n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.num_of_questions = 1\n",
      "        self.num_of_answers = 1\n",
      "        self.num_of_possible_responses = 4\n",
      "        self.num_of_possible_question_types = 2\n",
      "        self.num_of_possible_response_types = 1\n",
      "        self.', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 128, 'total_tokens': 184}}                                                                                                                              \n",
      "2  {'id': 'cmpl-657b46c1-6345-4e52-a0ee-f8ef95b0bf10', 'object': 'text_completion', 'created': 1689795492, 'model': '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\n",
      "    SPECIFICATIONS:\n",
      "        * Use the prompt below as a starting point\n",
      "        * Keep it under 350-words (500 words or less)\n",
      "        \n",
      "       A) What are your [X] keywords for natural language processing? I'm interested in them because they\n",
      "           form part of Hegel, a free-to-use suite of developer tools for large\n",
      "             models. I wonder if you've incorporated any of our\n",
      "             technology into the [Y] toolkit that you built to\n",
      "              train your core natural language processing models (ie:', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 92, 'completion_tokens': 128, 'total_tokens': 220}}   \n",
      "3  {'id': 'cmpl-d169a48b-a3f2-4314-86db-23d37463afc7', 'object': 'text_completion', 'created': 1689795517, 'model': '/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\n",
      "    PARAMTERS:\n",
      "        Question1 (String) - The full question \"Do You Offer Refunds?\"\n",
      "        <ResponseType>String</ResponseType>\n",
      "            default: <ResponseType string = \"No. We do not offer refunds as we are still developing the software.\">\n",
      "\n",
      "    SCORE SYNTAX:\n",
      "        \n",
      "    Q1(Question1) <ResponseType String>\n",
      "\"\"\"\n",
      "from unittest import TestCase, main\n",
      "import sys, os\n",
      "sys.path.append('../') \n",
      "import json, random\n",
      "import H', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 128, 'total_tokens': 184}}                                                                                                  \n",
      "4  {'id': 'cmpl-c8c6f449-ab91-4db5-bd43-1934ec1391fd', 'object': 'text_completion', 'created': 1689795552, 'model': '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\n",
      "    I am a developer at Hegel AI and I have been working on a project that uses the GPT-3 model.\n",
      "    I would like to know more about your company and how you are using the GPT-3 model in your work.\n",
      "    Can we schedule a 15 minute call for me to learn more?\n",
      "    \n",
      "    DATA:\n",
      "    - Hegel AI is a startup that builds developer tools for large language models.\n",
      "    - The prospect works at a company called XYZ and they are using the GPT-3 model in their work.\n",
      "   ', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 92, 'completion_tokens': 128, 'total_tokens': 220}}                                                            \n",
      "5  {'id': 'cmpl-179f9de5-7a05-44bf-8f1a-1cf668d2fc41', 'object': 'text_completion', 'created': 1689795599, 'model': '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\n",
      "    Yes, we do!\n",
      "    \n",
      "    EXPLANATION:\n",
      "    The answer is yes because the company offers refunds.\n",
      "    The answer is no because the company does not offer refunds.\n",
      "\"\"\"\n",
      "\n",
      "# 1. Write a function that takes in a string and returns True if it contains the word \"yes\".\n",
      "# 2. Write a function that takes in a string and returns True if it contains the word \"no\".\n",
      "# 3. Write a function that takes in a string and returns True if it contains the word \"offer\"\n",
      "# 4.', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 128, 'total_tokens': 184}}                                                                    \n",
      "6  {'id': 'cmpl-c50dde7a-f617-4e43-85c2-395b8777de58', 'object': 'text_completion', 'created': 1689795630, 'model': '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\n",
      "    Hegel AI is an online platform that offers developer tools to help users\n",
      "    train and test large language models like LAVaTT for speech synthesis\n",
      "    applications, like conversational chat bots (e.g., Siri, Alexa). \n",
      "\n",
      "    Our platform also lets users monitor and benchmark the\n",
      "    effectiveness of these models against standard NLP benchmarks.\n",
      "    \n",
      "    To book a demo, send an email to [*HegelAIDemo@hegelai.com*.](mailto:HegelAIDemo@hegelai.com)', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 92, 'completion_tokens': 128, 'total_tokens': 220}}                                                                   \n",
      "7  {'id': 'cmpl-7ede9837-32be-40bc-83b0-4b03e1338b60', 'object': 'text_completion', 'created': 1689795667, 'model': '/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin', 'choices': [{'text': '\n",
      "        Refunds are not available, but our free-to-use app (the “App”) may help to better understand how your child interacts with your mobile device.\n",
      "        15. We’re excited for you to take advantage of the App as a parent in order to  see what’s happening on screen and also be aware if something \n",
      "        happens when your child is not present (e.g., phone number, email, device name).\n",
      "        16. Please understand that Hegel AI does not offer refunds. If your purchase has already been processed for the App', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 128, 'total_tokens': 184}}   \n",
      "\n",
      "     latency  auto-evaluation   \n",
      "0  34.588089  0.0              \\\n",
      "1  24.209603  0.0               \n",
      "2  24.555184  0.0               \n",
      "3  35.197844  0.0               \n",
      "4  46.882386  0.0               \n",
      "5  30.860944  0.0               \n",
      "6  36.939122  0.0               \n",
      "7  30.535586  0.0               \n",
      "\n",
      "                                                   model_path  temperature  \n",
      "0  /Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin    0.0          \n",
      "1  /Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin    0.0          \n",
      "2  /Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin    1.0          \n",
      "3  /Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin    1.0          \n",
      "4  /Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin  0.0          \n",
      "5  /Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin  0.0          \n",
      "6  /Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin  1.0          \n",
      "7  /Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin  1.0          \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mharness\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse(s)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Development/prompttools/prompttools/harness/multi_experiment_harness.py:67\u001b[0m, in \u001b[0;36mMultiExperimentHarness.visualize\u001b[0;34m(self, colname)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m colname:\n\u001b[0;32m---> 67\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/reshape/pivot.py:97\u001b[0m, in \u001b[0;36mpivot_table\u001b[0;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[1;32m     94\u001b[0m     table \u001b[38;5;241m=\u001b[39m concat(pieces, keys\u001b[38;5;241m=\u001b[39mkeys, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43m__internal_pivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/reshape/pivot.py:166\u001b[0m, in \u001b[0;36m__internal_pivot_table\u001b[0;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(values)\n\u001b[0;32m--> 166\u001b[0m grouped \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m agged \u001b[38;5;241m=\u001b[39m grouped\u001b[38;5;241m.\u001b[39magg(aggfunc)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropna \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(agged, ABCDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agged\u001b[38;5;241m.\u001b[39mcolumns):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/frame.py:8256\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   8254\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m-> 8256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8259\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8262\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8266\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:931\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:985\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m    983\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 985\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m    988\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'models'"
     ]
    }
   ],
   "source": [
    "harness.visualize(\"response(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d483b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
