{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cfea46b",
   "metadata": {},
   "source": [
    "# Basic Experiment Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac35f8",
   "metadata": {},
   "source": [
    "## Setup imports and API keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edba05a",
   "metadata": {},
   "source": [
    "First, we'll need to set our API keys. If we are in DEBUG mode, we don't need to use real OpenAI or Hegel AI API keys, so for now we'll set them to empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed4e635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DEBUG']=\"1\"\n",
    "os.environ['HEGELAI_API_KEY'] = \"\"  # Optional, it will be needed to use with `HegelScribe` to persist/visualize your experiments\n",
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f1e47",
   "metadata": {},
   "source": [
    "Then we'll import the relevant `prompttools` modules to setup our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beaa70a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from prompttools.experiment.openai_chat_experiment import OpenAIChatExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622dea9a",
   "metadata": {},
   "source": [
    "## Run an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3babfe5a",
   "metadata": {},
   "source": [
    "Next, we create our test inputs. We can iterate over models, inputs, and configurations like temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9114cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt-3.5-turbo', 'gpt-3.5-turbo-0613']\n",
    "messages = [[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Who was the first president?\"},\n",
    "]]\n",
    "temperatures = [0.0, 1.0]\n",
    "\n",
    "experiment = OpenAIChatExperiment(models, messages, temperature=temperatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa5450",
   "metadata": {},
   "source": [
    "We can then run the experiment to get results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b33130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-05 15:34:24,014] INFO in experiment: Preparing first...\n"
     ]
    }
   ],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c13eb",
   "metadata": {},
   "source": [
    "## Evaluate the model response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb8023",
   "metadata": {},
   "source": [
    "To evaluate the results, we'll define an eval function. We can use semantic distance to check if the model's response is similar to our expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ddbb951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompttools.utils import similarity\n",
    "\n",
    "\n",
    "EXPECTED = {\"Who was the first president?\": \"George W\"}\n",
    "\n",
    "def extract_responses(output) -> str:\n",
    "    return [choice[\"message\"][\"content\"] for choice in output[\"choices\"]]\n",
    "\n",
    "\n",
    "def measure_similarity(\n",
    "    messages: List[Dict[str, str]], results: Dict, metadata: Dict\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    A simple test that checks semantic similarity between the user input\n",
    "    and the model's text responses.\n",
    "    \"\"\"\n",
    "    distances = [\n",
    "        similarity.compute(EXPECTED[messages[1][\"content\"]], response)\n",
    "        for response in extract_responses(results)\n",
    "    ]\n",
    "    return min(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d6065",
   "metadata": {},
   "source": [
    "Finally, we can evaluate and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e80dfeec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-05 15:34:24,125] INFO in posthog: Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
      "[2023-07-05 15:34:24,137] INFO in ctypes: Successfully imported ClickHouse Connect C data optimizations\n",
      "[2023-07-05 15:34:24,138] INFO in ctypes: Successfully import ClickHouse Connect C/Numpy optimizations\n",
      "[2023-07-05 15:34:24,144] INFO in json_impl: Using python library for writing JSON byte strings\n",
      "[2023-07-05 15:34:24,233] WARNING in Collection: No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
      "[2023-07-05 15:34:24,599] WARNING in Collection: No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
      "[2023-07-05 15:34:24,885] WARNING in Collection: No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
      "[2023-07-05 15:34:25,168] WARNING in Collection: No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "experiment.evaluate(\"similar_to_expected\", measure_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d09c18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>response(s)</th>\n",
       "      <th>latency</th>\n",
       "      <th>similar_to_expected</th>\n",
       "      <th>model</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who was the first president?'}]</td>\n",
       "      <td>[George Washington]</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.199477</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who was the first president?'}]</td>\n",
       "      <td>[George Washington]</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.199477</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who was the first president?'}]</td>\n",
       "      <td>[George Washington]</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.199477</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who was the first president?'}]</td>\n",
       "      <td>[George Washington]</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.199477</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                       messages   \n",
       "0  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who was the first president?'}]  \\\n",
       "1  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who was the first president?'}]   \n",
       "2  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who was the first president?'}]   \n",
       "3  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who was the first president?'}]   \n",
       "\n",
       "           response(s)   latency  similar_to_expected               model   \n",
       "0  [George Washington]  0.000003  0.199477             gpt-3.5-turbo       \\\n",
       "1  [George Washington]  0.000002  0.199477             gpt-3.5-turbo        \n",
       "2  [George Washington]  0.000001  0.199477             gpt-3.5-turbo-0613   \n",
       "3  [George Washington]  0.000001  0.199477             gpt-3.5-turbo-0613   \n",
       "\n",
       "   temperature  \n",
       "0  0.0          \n",
       "1  1.0          \n",
       "2  0.0          \n",
       "3  1.0          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0007a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
